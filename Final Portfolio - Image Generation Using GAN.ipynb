{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Generation using GAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Straight to the basics: Machine Learning\n",
    "\n",
    "Machine Learning is a branch of computer science that uses statistical techniques to give computer systems ability to \"learn\" with data. Here computer learning means the ability to progressively improve performance on any specific task without being specifically programed to do so. Machine learning is divided into two broad types:\n",
    "\n",
    "-  __Supervised Machine Learning:__\n",
    "    \n",
    "    In this type, we provide a lot of training data and train the model with it. With the trained logic and new data, we then make output predictions.\n",
    "    \n",
    "<img src=\"Supervised.jpeg\",width=800,height=800>\n",
    "                                         Supervised Machine Learning\n",
    "\n",
    "    \n",
    "    \n",
    "-  __Unsupervised Machine Learning:__\n",
    "    \n",
    "    Here, no training data is provided to the model i.e. we don't tell the model where to go, it has to understand itself from the unstructured data that we provide.\n",
    "    \n",
    "<img src=\"Unsupervised.jpeg\",width=800,height=800>\n",
    "                                         Unsupervised Machine Learning    \n",
    "    \n",
    "### Neural Network? What's that?\n",
    "\n",
    "Neural networks are computing systems somewhat inspired by the biological neural networks that constitute animal brains. Such systems \"learn\" tasks by considering examples, generally without task-specific programming. For instance, in image recognition, they might learn to identify images that contain humans by analyzing example images that have been manually labeled as \"human\" or \"not a human\" and using the results to identify human in other images. They do this without any a priori knowledge about humans. For example, the networks aren't aware of legs, hands, and human-like faces that we possess. Instead, they evolve their own set of relevant characteristics from the learning material that they process.\n",
    "\n",
    "Neural networks use a cascade of multiple layers of nonlinear processing unit for feature extraction and transformation. Each successive layer uses the output from the previous layer as input. It is a type of unsupervised learning like pattern analysis. A deep neural network is an artificial neural network with multiple hidden layers between the input and output layers. They can model complex non-linear relationships. The extra layers enable composition of features from lower layers, potentially modeling complex data with fewer units than a similarly performing shallow network. These are typically feedforward networks in which data flows from the input layer to the output layer without looping back.\n",
    "\n",
    " <img src=\"Deep Neural Network.jpeg\",width=600,height=600>\n",
    "                                              Deep Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction to GAN - Generative Adversarial Network\n",
    "Generative adversarial network (GAN) was introduced by Ian Goodfellow and his colleagues in 2014. GANs are a class of artificial intelligence algorithms used in unsupervised machine learning, implemented by a system of two neural networks. These two networks play against each other while co-training through the plain old backpropagation.\n",
    "\n",
    "GANs' potential is huge, because they can learn to mimic any kind of data distribution. That is, GANs can be taught to create worlds strangely similar to our own in various domains like images, music and speech. They are machine robot artists in a way, and their output is seriously impressive. Existing GANs have been mainly used for image generation tasks, with which they have showed impressive results and produced sharp and realistic images. Because of the flexible nature of the model definition and high-quality results, GANs have been applied to many real-world applications, including super-resolution, colorization, face generation, image completion, etc.\n",
    "\n",
    "### Working: Explain me like I am five!\n",
    "\n",
    "The working of a GAN lies within its name! Yes, its name! The word 'adversarial' is defined as characterized by conflict or opposition. The two networks that we mentioned earlier, contest with each other in a zero-sum game framework. Zero-sum game is a mathematical representation of a situation in which each network's gain or loss of utility is exactly balanced by the losses or gains of the utility of the other network. If the total gains of the networks are added up and the total losses are subtracted, they will sum to zero.\n",
    "\n",
    "Consider the below illustration.\n",
    "\n",
    "<img src=\"GAN Basic.jpeg\",width=800,height=800>\n",
    "\n",
    "\n",
    "Here, R is some real data set which is supplied to the model, G is the forger(generator) which is trying to generate fake images that looks just like the real-life image using the I which is random noise input, while D is Detective (discriminator) which is getting the generated images from the forger and real-life data set. Here, detective evaluates the fakes images and labels the difference. The fake labelled images are sent back to the forger via backpropagation. \n",
    "\n",
    "So, to sum up, G is trying to match real paintings with their output, while D is trying to tell the difference, except that in this case, the forger G never get to see the original data; just the judgments of D. Hence, over a period of time, both D and G will get better until G has become a master forger creating real-like images and D was unable to differentiate between the two.\n",
    "\n",
    "Another brief explanation from original GAN research paper:\n",
    "\n",
    "> The generative model can be thought of as analogous to a team of counterfeiters, trying to produce fake currency and use it without detection, while the discriminative model is analogous to the police, trying to detect the counterfeit currency. Competition in this game drives both teams to improve their methods until the counterfeits are indistinguishable from the genuine articles.\n",
    "\n",
    "To read a more about it, __[here](http://arxiv.org/pdf/1406.2661v1.pdf)__ is the link for the original paper introducing GAN.\n",
    "\n",
    "The most important roadblock while training a GAN is stability. If you start to train a GAN, and the discriminator part is much powerful that its generator counterpart, the generator would fail to train effectively. This will in turn affect training of your GAN. On the other hand, if the discriminator is too lenient; it would let literally any image be generated. And this will mean that your GAN is useless.\n",
    "\n",
    "\n",
    "GANs have been mainly used for image generation tasks, with which they have showed impressive results and produced sharp and realistic images. Because of the flexible nature of the model definition and high-quality results, GANs have been applied to many real-world applications, including super-resolution, colorization, face generation, image completion.\n",
    "\n",
    "\n",
    "### How GANs work\n",
    "One neural network, called the *generator*, generates new data instances, while the other, the *discriminator*, evaluates them for authenticity; i.e. the discriminator decides whether each instance of data it reviews belongs to the actual training dataset or not.\n",
    "\n",
    "<img src=\"GAN Archii.png\",width=800,height=800>\n",
    "\n",
    "### Here are the steps a GAN takes:\n",
    " - The generator takes in random numbers and returns an image\n",
    " - This generated image is fed into the discriminator alongside a stream of images taken from the actual dataset\n",
    " - The discriminator takes in both real and fake images and returns probabilities, a number between 0 and 1, with 1 representing a prediction of authenticity and 0 representing fake\n",
    " \n",
    "<img src=\"GAN Working.png\",width=700,height=700>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why are we using Keras?\n",
    "Keras is a high-level neural networks API, written in Python and capable of running on top of TensorFlow.\n",
    "It was developed with a focus on enabling fast experimentation.\n",
    "Allows for easy and fast prototyping (through user friendliness, modularity, and extensibility).\n",
    "Supports both convolutional networks and recurrent networks, as well as combinations of the two.\n",
    "Runs seamlessly on CPU and GPU.\n",
    "We don’t have to declare any weights or bias variables like we do in TensorFlow, Keras sorts that out for us."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Two Networks of GAN\n",
    "To understand GANs, you should know how generative algorithms work, and for that, contrasting them with discriminative algorithms is instructive. Discriminative algorithms try to classify input data; that is, given the features of a data instance, they predict a label or category to which that data belongs.\n",
    "\n",
    "#### 1. Generator\n",
    "        \n",
    "  The structure and the layers of Generator is as below:\n",
    "  1. It takes noise as input to generate image.\n",
    "  2. The Sequential model is a linear stack of layers. You can create a Sequential model by simply add layers via the .add() method.\n",
    "  3. Add a Dense layer. Dense layer is nothing but the Neurons fully connected with another layer. It takes Noise as input in linear format.\n",
    "  4. Using LeakyReLU a non-linear rectified activation function to work well, especially for higher resolution modeling.\n",
    "  5. Using BatchNormalization to convert input into linear array for the next dense layer\n",
    "  6. Repeating these 3 layers and increasing size of Dense layer each time\n",
    "  7. Creating Dense layer of size equal to total number of pixels\n",
    "  8. Reshape gives a new shape to an array without changing any data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_generator(self):\n",
    "\n",
    "        noise_shape = (100,)                           #Takes noise as input to generate image\n",
    "        \n",
    "        model = Sequential()                           #Linear stack of layers\n",
    "\n",
    "        model.add(Dense(256, input_shape=noise_shape)) #Fully connected layer (256 Neurons) taking Noise as input in linear format\n",
    "        model.add(LeakyReLU(alpha=0.2))                #Passing linear data to non-linear activation function\n",
    "        model.add(BatchNormalization(momentum=0.8)) ,  #Normalize data to forward to the next linear layer\n",
    "        model.add(Dense(512))                          #512 Neurons\n",
    "        model.add(LeakyReLU(alpha=0.2))                #leaky rectified activation works well, especially for higher resolution modeling\n",
    "        model.add(BatchNormalization(momentum=0.8))    #Gradiant Descent - to minimize the losses\n",
    "        model.add(Dense(1024))                         #1024 Neurons\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Dense(2048))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Dense(np.prod(self.img_shape), activation='tanh')) #Creating Dense layer of size equal to total number of pixels\n",
    "        model.add(Reshape(self.img_shape))             #Gives a new shape to an array without changing any data\n",
    "\n",
    "        model.summary()                                #prints a summary representation of your model\n",
    "\n",
    "        noise = Input(shape=noise_shape)\n",
    "        img = model(noise)\n",
    "\n",
    "        return Model(noise, img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Discriminator\n",
    "The structure and the layers of Generator is as below:\n",
    "  1. It takes generated image as input to validate. Something like this:\n",
    "<img src=\"GAN_mnist_0.png\",width=300,height=300>\n",
    "  2. The Sequential model is a linear stack of layers. You can create a Sequential model by simply add layers via the .add() method.\n",
    "  3. Flattens the image in linear array\n",
    "  4. Fully connected layer (512 Neurons) taking generated image as input in linear format\n",
    "  5. Returns the output between 0(Fake) to 1(Real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_discriminator(self):\n",
    "\n",
    "        img_shape = (self.img_rows, self.img_cols, self.channels)\n",
    "        \n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Flatten(input_shape=img_shape))        #Flattens the image in linear array\n",
    "        model.add(Dense(512))                            #Fully connected layer (512 Neurons) taking generated image as input in linear format\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dense(256))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dense(1, activation='sigmoid'))        #Returns the output between 0 to 1\n",
    "        model.summary()\n",
    "\n",
    "        img = Input(shape=img_shape)\n",
    "        validity = model(img)                            #Returns the validity of the image passed to the generator\n",
    "\n",
    "        return Model(img, validity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to run the GAN on your machine\n",
    "\n",
    "To run this .ipynb notebook on your machine you need below things installed:\n",
    "\n",
    " - Jupyter Notebook\n",
    " - python 3\n",
    " - Tensorflow (For Installation instruction click __[here](https://www.tensorflow.org/install/install_windows)__.)\n",
    " - Keras (For Installation instruction click __[here](https://keras.io/)__.)\n",
    " - Some basic libraries such as numpy, matplotlib\n",
    " - All the other packages will be imported using keras\n",
    " \n",
    "Once you complete above requirements simply run this notebook. It will automatically import the MNIST dataset from keras.datasets. The generated images will be stored on your machine in the folder where you stored this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Steps to run the notebook on your machine\n",
    "\n",
    "Step 1: Import keras and all the layers requires to create the Discriminator and the Generator Networks\n",
    "\n",
    "Step 2: Import numpy and matplotlib to perform numeric operations and ploting graphs\n",
    "\n",
    "Step 3: Simply add the Number of Epochs, batch size and the interval after which you want to save the generated image\n",
    "\n",
    "Step 4: Run the cell, it will start processing and at the end plot the graphs of Discriminator loss vs number of epochs and Generator loss vs number of epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout\n",
    "from keras.layers import BatchNormalization, Activation, ZeroPadding2D\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.optimizers import Adagrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class GAN():\n",
    "    def __init__(self):\n",
    "        self.img_rows = 28 \n",
    "        self.img_cols = 28\n",
    "        self.channels = 1 #For black and white 1, for color 3\n",
    "        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n",
    "\n",
    "        optimizer = Adam(0.0002, 0.5) #Adam optimizer is Appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients\n",
    "        #lower the learning rate the more reliable is the training\n",
    "        # Build and compile the discriminator\n",
    "        self.discriminator = self.build_discriminator()\n",
    "        self.discriminator.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "        #A metric is a function that is used to judge the performance of your model. Metric functions are to be \n",
    "        #supplied in the  metrics parameter when a model is compiled.\n",
    "\n",
    "        # Build and compile the generator\n",
    "        self.generator = self.build_generator()\n",
    "        self.generator.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "\n",
    "        # The generator takes noise as input and generated imgs\n",
    "        z = Input(shape=(100,))\n",
    "        img = self.generator(z)\n",
    "\n",
    "        # For the combined model we will only train the generator\n",
    "        self.discriminator.trainable = False\n",
    "\n",
    "        # The valid takes generated images as input and determines validity\n",
    "        valid = self.discriminator(img)\n",
    "\n",
    "        # The combined model  (stacked generator and discriminator) takes\n",
    "        # noise as input => generates images => determines validity \n",
    "        self.combined = Model(z, valid)\n",
    "        self.combined.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "        \n",
    "    def build_discriminator(self):\n",
    "\n",
    "        img_shape = (self.img_rows, self.img_cols, self.channels)\n",
    "        \n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Flatten(input_shape=img_shape))        #Flattens the image in linear array\n",
    "        model.add(Dense(512))                            #Fully connected layer (512 Neurons) taking generated image as input in linear format\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dense(256))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dense(1, activation='sigmoid'))        #Returns the output between 0 to 1\n",
    "        model.summary()\n",
    "\n",
    "        img = Input(shape=img_shape)\n",
    "        validity = model(img)                            #Returns the validity of the image passed to the generator\n",
    "\n",
    "        return Model(img, validity)\n",
    "    \n",
    "    def build_generator(self):\n",
    "\n",
    "        noise_shape = (100,)\n",
    "        \n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Dense(256, input_shape=noise_shape))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Dense(512))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Dense(1024))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Dense(2048))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Dense(np.prod(self.img_shape), activation='tanh'))\n",
    "        model.add(Reshape(self.img_shape))\n",
    "\n",
    "        model.summary()\n",
    "\n",
    "        noise = Input(shape=noise_shape)\n",
    "        img = model(noise)\n",
    "\n",
    "        return Model(noise, img)\n",
    "\n",
    "    def train(self, epochs, batch_size=128, save_interval=50):\n",
    "\n",
    "        # Load the dataset\n",
    "        (X_train, _), (_, _) = mnist.load_data()\n",
    "\n",
    "        \n",
    "        X_train = (X_train.astype(np.float32) - 127.5) / 127.5  #Rescaling input array to -1 to 1\n",
    "        X_train = np.expand_dims(X_train, axis=3)               #Expads image adding new axis\n",
    "\n",
    "        half_batch = int(batch_size / 2)\n",
    "        \n",
    "        #create 2 lists of discriminator loss and generator loss to plot the graph\n",
    "        d_loss_list = []\n",
    "        g_loss_list = []\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            # Train Discriminator\n",
    "            # Select a random half batch of images, adding 0 to prevent overfit\n",
    "            idx = np.random.randint(0, X_train.shape[0], half_batch)\n",
    "            imgs = X_train[idx]\n",
    "\n",
    "            noise = np.random.normal(0, 1, (half_batch, 100))\n",
    "\n",
    "            # Generate a half batch of new images\n",
    "            gen_imgs = self.generator.predict(noise)\n",
    "\n",
    "            # Train the discriminator\n",
    "            d_loss_real = self.discriminator.train_on_batch(imgs, np.ones((half_batch, 1)))\n",
    "            d_loss_fake = self.discriminator.train_on_batch(gen_imgs, np.zeros((half_batch, 1)))\n",
    "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "            # Train Generator\n",
    "            noise = np.random.normal(0, 1, (batch_size, 100))\n",
    "\n",
    "            # The generator wants the discriminator to label the generated samples as valid (ones)\n",
    "            valid_y = np.array([1] * batch_size)\n",
    "\n",
    "            # Train the generator\n",
    "            g_loss = self.combined.train_on_batch(noise, valid_y)\n",
    "\n",
    "            # Plot the progress\n",
    "            print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss))\n",
    "            #Adding the losses into lists for plotting graph\n",
    "            d_loss_list.append(d_loss[0])\n",
    "            g_loss_list.append(g_loss)\n",
    "            \n",
    "            # If at save interval => save generated image samples\n",
    "            if epoch % save_interval == 0:\n",
    "                self.save_imgs(epoch)\n",
    "        #Plot the graph of discriminator loss vs Number of epochs\n",
    "        plt.rcParams['figure.figsize'] = (8, 4)\n",
    "        plt.plot(d_loss_list,label=\"epoch/d_loss\")\n",
    "        plt.title('Discriminator Loss')\n",
    "        plt.ylabel('d_loss')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.show()\n",
    "        \n",
    "        #Plot the graph of Generator loss vs Number of epochs\n",
    "        plt.rcParams['figure.figsize'] = (8, 4)\n",
    "        plt.plot(g_loss_list,label=\"epoch/g_loss\")\n",
    "        plt.title('Generator Loss')\n",
    "        plt.ylabel('g_loss')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.show()\n",
    "        \n",
    "    def save_imgs(self, epoch):\n",
    "        r, c = 5, 5\n",
    "        noise = np.random.normal(0, 1, (r * c, 100)) #Giving the size of row and columns for images\n",
    "        gen_imgs = self.generator.predict(noise)\n",
    "\n",
    "        # Rescale images 0 - 1\n",
    "        gen_imgs = 0.5 * gen_imgs + 0.5\n",
    "\n",
    "        fig, axs = plt.subplots(r, c)\n",
    "        cnt = 0\n",
    "        for i in range(r):\n",
    "            for j in range(c):\n",
    "                axs[i,j].imshow(gen_imgs[cnt, :,:,0], cmap='gray')\n",
    "                axs[i,j].axis('off')\n",
    "                cnt += 1\n",
    "        fig.savefig(\"GAN_mnist_%d.png\" % epoch)\n",
    "        plt.close()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    gan = GAN()\n",
    "    gan.train(epochs=30000, batch_size=64, save_interval=200) #Giving number of Epochs, Batch Size and the iterval after which we want to save the image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code Reference\n",
    "\n",
    "The code in the document by deeplearning4j/deeplearning4j is licensed under the Apache License 2.0\n",
    "\n",
    "We added an extra layer in generator to improve the output. Plotting the graph of Accuracy and epochs. Also tuned some of the hyper parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Generated by GAN\n",
    "Input :- \n",
    "Number of Epochs - 30000\n",
    "Batch Size - 64\n",
    "Saving Interval - 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                                                Image Generated at 29600 the Epoch\n",
    "<img src=\"mnist_29600.png\",width=400,height=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Graph of Discrimintor Loss and Generator Loss\n",
    "\n",
    "<img src=\"GAN Discriminator Loss.png\",width=600,height=400>\n",
    "\n",
    "<img src=\"GAN Generator Loss.png\",width=600,height=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why we use Activation functions with Neural Networks?\n",
    "\n",
    "It is used to determine the output of neural network like yes or no. It maps the resulting values in between 0 to 1 or -1 to 1 etc. (depending upon the function).\n",
    "\n",
    "#### Acivation Functions used in GAN\n",
    "\n",
    " __1. Sigmoid or Logistic Activation Function__\n",
    "The Sigmoid Function curve looks like a S-shape.\n",
    "<img src=\"Sigmoid Activation Function.png\",width=600,height=400>\n",
    "\n",
    "The main reason why we use sigmoid function is because it exists between (0 to 1). Therefore, it is especially used for models where we have to __predict the probability__ as an output. Since probability of anything exists only between the range of __0 and 1__, sigmoid is the right choice.\n",
    "\n",
    "**The Discriminator in GAN predicts the authenticity of the generated image and gives output as 0 or 1. Hence for Discriminator we are using Sigmoid as an Activation function.**\n",
    "\n",
    " __2. Tanh or hyperbolic tangent Activation Function__\n",
    "\n",
    "tanh is also like logistic sigmoid but better. The range of the tanh function is from (-1 to 1). tanh is also sigmoidal (s - shaped).\n",
    "\n",
    "<img src=\"Tanh VS Sigmoid Activation Function.png\",width=600,height=400>\n",
    "\n",
    "The advantage is that the negative inputs will be mapped strongly negative and the zero inputs will be mapped near zero in the tanh graph. The tanh function is mainly used classification between two classes.\n",
    "\n",
    " __3. Leaky ReLU__\n",
    "\n",
    "The issue with ReLU is that all the negative values become zero immediately which decreases the ability of the model to fit or train from the data properly. That means any negative input given to the ReLU activation function turns the value into zero immediately in the graph, which in turns affects the resulting graph by not mapping the negative values appropriately.\n",
    "It is an attempt to solve the dying ReLU problem\n",
    "<img src=\"ReLU VS LeakyReLU Activation Function.png\",width=600,height=400>\n",
    "\n",
    "The leak helps to increase the range of the ReLU function.Usually, the value of a is 0.01 or so.\n",
    "\n",
    "Therefore the range of the Leaky ReLU is (-infinity to infinity)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DCGAN - Deep Convolution Generative Adverserial Network\n",
    "\n",
    "The Deep Convolution Generative Adverserial Network (DCGAN) is an architecture for learning to generate new content just like GAN, also, it consists of a generator and discriminator. The only difference here is that DCGAN uses convolutional layers in both the networks, generator and discriminator.\n",
    "\n",
    "\n",
    "<img src=\"DCGAN Archii.jpeg\",width=800,height=800>\n",
    "\n",
    "(Fig. DCGAN Architecture)\n",
    "\n",
    "The first layer in the Generator is a fully connected layer which is reshaped into a deep and narrow layer. Then we use batch normalization and a leaky ReLU activation. Next is a transposed convolution where we would halve the depth and double the width and height of the previous layer. Again, we use batch normalization and leaky ReLU. For each of these layers, the general scheme is Convolution > Batch Normalization > Leaky ReLU.\n",
    "\n",
    "Discriminator is basically just a convolutional classifier. Use batch normalization on each layer except the first convolutional and output layers. Again, each layer looks something like Convolution > Batch Normalization > Leaky ReLU. So, it takes real or fake MNIST digits and applies a series of convolutions. Then we use a sigmoid to make sure our output can be interpreted as the probability the input image is a real MNIST character.\n",
    "\n",
    "\n",
    "\n",
    "To read the original paper introducing DCGANs, click __[here](https://arxiv.org/abs/1511.06434)__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Reshape\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D, MaxPooling2D\n",
    "from keras.layers.advanced_activations import LeakyReLU, ELU\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import Flatten, Dropout\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import numpy as np\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generator(input_dim=100,units=1024,activation='relu'):  #Defining parameters for the generator\n",
    "    \n",
    "    model = Sequential()                                    #Linear stack of layers\n",
    "    model.add(Dense(input_dim=input_dim, units=units))      #Fully connected layer (1024 Neurons) taking Noise as input\n",
    "    model.add(BatchNormalization())                         #Normalize data to forward to the next linear layer\n",
    "    model.add(Activation(activation))                       #Passing linear data to non-linear activation function relu\n",
    "    model.add(Dense(128*7*7))                               #Adding the next fully connected dense layer (128 neurons)\n",
    "    model.add(BatchNormalization())                         #Normalized data again so that it can be fed as input to next layer\n",
    "    model.add(Activation(activation))                       \n",
    "    model.add(Reshape((7, 7, 128), input_shape=(128*7*7,))) #Reshaping tensor to 7*7*128 so that we can upscale it\n",
    "    model.add(UpSampling2D((2, 2)))                         #used to double the rows and columns of input tensor, 14*14 currently\n",
    "    model.add(Conv2D(64, (5, 5), padding='same'))           #this layer creates kernel of (5x5) which is moving window\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(activation))\n",
    "    model.add(UpSampling2D((2, 2)))                         #Upsampled once again, 28*28 is the current required shape\n",
    "    model.add(Conv2D(1, (5, 5), padding='same'))\n",
    "    model.add(Activation('tanh'))                           #Final layer uses tanh to learn more quickly to saturate and cover the color space \n",
    "    print(model.summary())\n",
    "    return model\n",
    "\n",
    "def discriminator(input_shape=(28, 28, 1),nb_filter=64):    #Defined the input shape and Number of convolution filters to use.\n",
    "    model = Sequential()                                    #Linear stack of layers\n",
    "    model.add(Conv2D(nb_filter, (5, 5), strides=(2, 2),     #Adding convolution layer with (5x5) kernel\n",
    "                     padding='same',input_shape=input_shape))\n",
    "    model.add(BatchNormalization())                         #used to normalize the data as per the range of activation fn\n",
    "\n",
    "    model.add(ELU())                                        #higher classification accuracies, gets rid of vanishing gradient prob\n",
    "    model.add(Conv2D(2*nb_filter, (5, 5), strides=(2, 2)))\n",
    "    model.add(BatchNormalization())                         #used to normalize the data as per the range of activation fn\n",
    "    model.add(ELU())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))               #it is used to down scale by factor (2,2)\n",
    "    model.add(Flatten())                                    #Flattens the image in linear array\n",
    "    model.add(Dense(4*nb_filter))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.5))                                 #used to prevent overfitting during training\n",
    "    model.add(ELU())\n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation('sigmoid'))                        #to get output as 0 or 1\n",
    "    print(model.summary())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def combine_images(generated_images):\n",
    "    total,width,height = generated_images.shape[:-1]\n",
    "    cols = int(math.sqrt(total))\n",
    "    rows = math.ceil(float(total)/cols)\n",
    "    combined_image = np.zeros((height*rows, width*cols),dtype=generated_images.dtype)\n",
    "\n",
    "    for index, image in enumerate(generated_images):\n",
    "        i = int(index/cols)\n",
    "        j = index % cols\n",
    "        combined_image[width*i:width*(i+1), height*j:height*(j+1)] = image[:, :, 0]\n",
    "    return combined_image\n",
    "\n",
    "def show_progress(e,i,g0,d0,g1,d1):\n",
    "    sys.stdout.write(\"\\repoch: %d, batch: %d, g_loss: %f, d_loss: %f, g_accuracy: %f, d_accuracy: %f\" % (e,i,g0,d0,g1,d1))\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from keras.datasets import mnist\n",
    "from PIL import Image\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import SGD, Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (15, 5)\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "NUM_EPOCH = 60\n",
    "LR = 0.0002  # initial learning rate\n",
    "B1 = 0.5  # momentum term\n",
    "GENERATED_IMAGE_PATH = 'images/'\n",
    "GENERATED_MODEL_PATH = 'models/'\n",
    "\n",
    "#create 2 lists of discriminator loss and generator loss to plot the graph\n",
    "g_loss_list = []\n",
    "d_loss_list = []\n",
    "\n",
    "def train():\n",
    "    (X_train, y_train), (_, _) = mnist.load_data()\n",
    "    # normalize images\n",
    "    X_train = (X_train.astype(np.float32) - 127.5)/127.5\n",
    "    X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], X_train.shape[2], 1)\n",
    "\n",
    "    # build GAN\n",
    "    g = generator()\n",
    "    d = discriminator()\n",
    "\n",
    "    opt = Adam(lr=LR,beta_1=B1)                     #lr is learning rate given to th optimizer, beta is always between 0 & 1\n",
    "    d.trainable = True\n",
    "    d.compile(loss='binary_crossentropy', metrics=['accuracy'], optimizer=opt)\n",
    "    d.trainable = False\n",
    "    dcgan = Sequential([g, d])\n",
    "    opt= Adam(lr=LR,beta_1=B1)\n",
    "    dcgan.compile(loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'],\n",
    "                  optimizer=opt)\n",
    "\n",
    "    num_batches = int(X_train.shape[0] / BATCH_SIZE)\n",
    "    # create directory\n",
    "    if not os.path.exists(GENERATED_IMAGE_PATH):\n",
    "        os.mkdir(GENERATED_IMAGE_PATH)\n",
    "    if not os.path.exists(GENERATED_MODEL_PATH):\n",
    "        os.mkdir(GENERATED_MODEL_PATH)\n",
    "\n",
    "    print(\"-------------------\")\n",
    "    print(\"Total epoch:\", NUM_EPOCH, \"Number of batches:\", num_batches)\n",
    "    print(\"-------------------\")\n",
    "    z_pred = np.array([np.random.uniform(-1,1,100) for _ in range(49)]) #array to hold individual digit\n",
    "    y_g = [1]*BATCH_SIZE                                                #input image from g to d\n",
    "    y_d_true = [1]*BATCH_SIZE                                           #classified as 1 if image is real  \n",
    "    y_d_gen = [0]*BATCH_SIZE                                            #classified as 0 if image is fake\n",
    "    for epoch in list(map(lambda x: x+1,range(NUM_EPOCH))):\n",
    "        for index in range(num_batches):\n",
    "            X_d_true = X_train[index*BATCH_SIZE:(index+1)*BATCH_SIZE]                 #discriminator predicting\n",
    "            X_g = np.array([np.random.normal(0,0.5,100) for _ in range(BATCH_SIZE)])  #and suggesting improvements\n",
    "            X_d_gen = g.predict(X_g, verbose=0)                                       #to generator\n",
    "\n",
    "            # train discriminator\n",
    "            d_loss = d.train_on_batch(X_d_true, y_d_true)\n",
    "            d_loss = d.train_on_batch(X_d_gen, y_d_gen)\n",
    "            # train generator\n",
    "            g_loss = dcgan.train_on_batch(X_g, y_g)\n",
    "            show_progress(epoch,index,g_loss[0],d_loss[0],g_loss[1],d_loss[1]) #Prints progress to output screen\n",
    "            #Adding the losses into lists for plotting graph\n",
    "            d_loss_list.append(d_loss[1])\n",
    "            g_loss_list.append(g_loss[1])\n",
    "\n",
    "        # save generated images\n",
    "        image = combine_images(g.predict(z_pred))                 #passing values to create an output image which contains 7*7 digits\n",
    "        image = image*127.5 + 127.5                               #scaling back to original form\n",
    "        Image.fromarray(image.astype(np.uint8))\\\n",
    "            .save(GENERATED_IMAGE_PATH+\"%03depoch.png\" % (epoch))\n",
    "        \n",
    "        #Plot the garph of generator loss vs number of batches\n",
    "        plt.plot(g_loss_list,label=\"Generator loss\")\n",
    "        plt.title('Generator loss')\n",
    "        plt.ylabel('g_loss')\n",
    "        plt.xlabel('batches')\n",
    "        plt.show()\n",
    "        \n",
    "        #Plot the garph of discriminator loss vs number of batches\n",
    "        plt.plot(d_loss_list,label=\"Discriminator loss\")\n",
    "        plt.title('Discriminator loss')\n",
    "        plt.ylabel('d_loss')\n",
    "        plt.xlabel('batches')\n",
    "        plt.show()\n",
    "\n",
    "        print()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Code Reference\n",
    "https://github.com/vwrs/dcgan-mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Generated by DCGAN\n",
    "                                                Image Generated at 54th Epoch\n",
    "<img src=\"054epoch.png\",width=300,height=300>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The learning curve of loss functions.\n",
    " - The discriminator D’s LS is goes down and back up, as expected.\n",
    " - The generator’s LS is high initially and goes down as it learns the image data distribution. \n",
    " - At the same time the D’s loss rises. Both behave as expected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Graph of Discrimintor Loss and Generator Loss\n",
    "\n",
    "<img src=\"DCGAN Discriminator Loss.png\",width=600,height=400>\n",
    "\n",
    "<img src=\"DCGAN Generator Loss.png\",width=600,height=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Results\n",
    "\n",
    "#### Visual Comparison:\n",
    "\n",
    "<img src=\"mnist_29600.png\",width=410,height=400>  \n",
    ">                                    Image Generated by GAN at 29600 Epoch\n",
    "\n",
    "<img src=\"054epoch.png\",width=300,height=300>\n",
    ">                                    Image Generated by DCGAN at 54 Epoch\n",
    "\n",
    "\n",
    "From the above outputs, we can see that DCGAN did a tremendous work in image generation as compared to simple GAN. The images generated by DCGAN are much clearer, have less noise around the digits and the digits are easily recognizable. These images have a better resolution as compared to GAN and this was only possible because of the convolution-deconvolution layers present in the DCGAN. \n",
    "\n",
    "Also, we can see that the image generated by GAN is at 29600th epoch whereas the DCGAN generated image is just at 54th! DCGAN architecture is pretty complex when compared to vanilla GANs'. And because of this, it takes a huge processing power to generate the outputs. Its takes a high-end Graphics Processing Unit(GPU) if we want the network to perform fast. So, DCGAN generates better images at the expense of time factor. This is where GAN comes handy. If we don't have enough resources, we can use GAN and try to tweak its parameters over and over till we get acceptable results. But overall, if we want quality, DCGAN will be the winner everytime."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "We started to work on this topic by referring different research papers and technical articles on GAN and DCGAN, and we learnt that the DCGAN generates images with better quality as compared to the vanilla GAN. To test this, we began to learn and understand the detailed architecture and working of GAN and DCGAN, ultimately implementing it on the MNIST dataset. Furthermore, we also studied and compared the results of both the networks using graphs and output images. Since the evaluation of the quality of generated images is subjective, we will let ourselves be human discriminators and make statements on whether the generated images look recognizable and elegant to us. Thus by comparing results i.e. images generated by both the networks, we observed that the images generated by the DCGAN have overall better quality than generated by GAN, thus, confirming our learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Licenses\n",
    "\n",
    "#### MIT License\n",
    "\n",
    "Copyright 2018 Amogh Chakkarwar\n",
    "\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n",
    "\n",
    "The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n",
    "\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creative Commons License\n",
    "\n",
    "The text in the document by Amogh Chakkarwar is licensed under CC BY 3.0 https://creativecommons.org/licenses/by/3.0/us/\n",
    "\n",
    "<a rel=\"license\" href=\"http://creativecommons.org/licenses/by/3.0/us/\"><img alt=\"Creative Commons License\" style=\"border-width:0\" src=\"https://i.creativecommons.org/l/by/3.0/us/88x31.png\" /></a><br />This work is licensed under a <a rel=\"license\" href=\"http://creativecommons.org/licenses/by/3.0/us/\">Creative Commons Attribution 3.0 United States License</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    " - https://keras.io/ (Keras)\n",
    " - https://hackernoon.com/how-do-gans-intuitively-work-2dda07f247a1 (How does GANs Work)\n",
    " - https://stackoverflow.com/questions/ (Basic questions related to python)\n",
    " - https://arxiv.org/abs/1511.07289  (Activation Function Elu)\n",
    " - https://github.com/vwrs/dcgan-mnist (DCGAN Code Reference)\n",
    " - https://deeplearning4j.org/generative-adversarial-network (GAN)\n",
    " - https://towardsdatascience.com/activation-functions-neural-networks-1cbd9f8d91d6 (Activation Functions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
